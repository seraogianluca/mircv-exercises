{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Exercise5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seraogianluca/mircv-exercises/blob/main/Exercise5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghEj0gcw3-63"
      },
      "source": [
        "#MIRCV 2021\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQvxakE6GVvA"
      },
      "source": [
        "#To support GPU\n",
        "!pip install opencv-python==4.4.0.46"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9UD8-oK5a8h"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output, display\n",
        "from imutils.video import VideoStream, FileVideoStream\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "BASE_DIR = '/content/gdrive/My Drive/mircv2021/data/facerecognition'\n",
        "\n",
        "#Detector parameters\n",
        "DET_PROTO = BASE_DIR + '/caffe/face_detector/deploy.prototxt'\n",
        "DET_MODEL = BASE_DIR + '/caffe/face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
        "DET_LAYER = 'detection_out'\n",
        "DET_SIZE = (300, 300)\n",
        "DET_MEAN = (123, 177,104)\n",
        "DET_THRESHOLD = 0.7\n",
        "\n",
        "#VGG2 parameters\n",
        "FEAT_PROTO = BASE_DIR + '/caffe/face_features/resnet50_ft.prototxt'\n",
        "FEAT_MODEL = BASE_DIR + '/caffe/face_features/resnet50_ft.caffemodel'\n",
        "FEAT_LAYER = \"pool5/7x7_s1\"\n",
        "FEAT_SIZE = (224, 224)\n",
        "FEAT_MEAN = (91.4953, 103.8827, 131.0912)\n",
        "PRED_THRESHOLD = 0.7\n",
        "\n",
        "SRC_FOLDER = BASE_DIR + '/classes'\n",
        "\n",
        "#k-Nearest Neighbors\n",
        "VIDEO_PATH = BASE_DIR +  \"/videos/dimartedi.mp4\"\n",
        "FRAMES_TO_SKIP = 800\n",
        "\n",
        "GREEN = (0, 255, 0)\n",
        "RED = (0, 0, 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRZkBI231LV"
      },
      "source": [
        "def highlight(img, rect, color, text=None):\n",
        "    cv2.rectangle(img, rect[0], rect[1], color, 3, 3, 0)\n",
        "    point = (rect[0][0] - 20, rect[1][1] + 50)\n",
        "    if text != None:\n",
        "        cv2.putText(img, text, point, 4, 1.2, color)\n",
        "\n",
        "\n",
        "def getImageROI(img, face):\n",
        "    return img[ face[0][1]:face[1][1], face[0][0]:face[1][0]]\n",
        "\n",
        "\n",
        "def display_img(img, is_bgr=True):\n",
        "  if is_bgr:  # convert color from CV2 BGR to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \n",
        "  display(Image.fromarray(img))\n",
        "  clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcUTxBtb31K3"
      },
      "source": [
        "class DNNExtractor:    \n",
        "    \n",
        "    def __init__(self, net_proto_path, trained_model_path, size, mean_values=None):\n",
        "        self.size = size\n",
        "        self.mean_values = mean_values\n",
        "\n",
        "        self.net = cv2.dnn.readNetFromCaffe(net_proto_path, trained_model_path)\n",
        "        # to enable GPU (this won't work on Colab without recompiling opencv)\n",
        "        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "        \n",
        "    \n",
        "    def extract(self, img, layer, normalize=False):\n",
        "        blob = cv2.dnn.blobFromImage(img, 1.0, self.size, self.mean_values, swapRB=False, crop=False)\n",
        "        self.net.setInput(blob)\n",
        "        prob = self.net.forward(layer).flatten()\n",
        "\n",
        "        if normalize:\n",
        "            prob /= np.linalg.norm(prob)\n",
        "\n",
        "        return prob\n",
        "    \n",
        "# it creates an instance of the class DNNExtractor\n",
        "det = DNNExtractor(DET_PROTO, DET_MODEL, DET_SIZE, DET_MEAN)\n",
        "fe = DNNExtractor(FEAT_PROTO, FEAT_MODEL, FEAT_SIZE, FEAT_MEAN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6hIxMUUMPF5"
      },
      "source": [
        "def extract_features(img_folder):\n",
        "    # find all jpgs inside a subfolder\n",
        "    img_paths = glob.glob(SRC_FOLDER + \"/*/*.jpg\")\n",
        "    img_paths = sorted(img_paths)\n",
        "\n",
        "    # get the id from filename: 22301.lavia.jpg --> lavia\n",
        "    ids = [os.path.basename(path).split('.')[1] for path in img_paths]\n",
        "\n",
        "    # extract features\n",
        "    descs = [fe.extract(cv2.imread(path), FEAT_LAYER, normalize=True) for path in tqdm(img_paths)]\n",
        "\n",
        "    return np.array(descs), np.array(ids)\n",
        "\n",
        "descs, ids = extract_features(SRC_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqi5DMi731Le"
      },
      "source": [
        "#TODO: STEP 1 - Detection\n",
        "\n",
        "#detection is a vector of values\n",
        "#please note that the detection coordinates are normalized. Revert back to the original coordinate system before putting them on the faces\n",
        "#[batchId, classId, confidence, left, top, right, bottom]\n",
        "def detect(img):\n",
        "  bbs = det.extract(img, DET_LAYER)\n",
        "  bbs = bbs.reshape(-1, 7)  # one detection per row\n",
        "  h,w, _ = img.shape\n",
        "    \n",
        "  faces = []\n",
        "\n",
        "  for batch_id, class_id, confidence, left, top, right, bottom in bbs:\n",
        "    #check if the confidence value <  DET_THRESHOLD\n",
        "    if confidence > DET_THRESHOLD:\n",
        "    #determine points p0 and p1, i.e. the top left and bottom right vertex of the BB, with respect to the original coordinates from the normalized dimensions: left, top, right, bottom \n",
        "      p0 = (int(left*w), int(top*h))\n",
        "      p1 = (int(right*w), int(bottom*h))\n",
        "      faces.append((p0,p1))\n",
        "  return faces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSKMrY3Q31Lk"
      },
      "source": [
        "#TODO: STEP 2 - Bounding Boxes\n",
        "img = cv2.imread(os.path.join(BASE_DIR, 'lenna.jpg'))\n",
        "faces = detect(img)\n",
        "#for each face use highlight function to draw the face BB\n",
        "for face in faces:\n",
        "  highlight(img, face, GREEN, None)\n",
        "\n",
        "#call display_img to display the detected faces\n",
        "display_img(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvnlnOb731LO"
      },
      "source": [
        "#TODO: STEP 3 - 1NN Classifier\n",
        "\n",
        "class OneNNClassifier:\n",
        "    \n",
        "  def __init__(self, ids, descs):\n",
        "      self.ids = ids\n",
        "      self.descs = descs\n",
        "\n",
        "  def predict(self, queryF):\n",
        "    #evaluate the dot products between descs and queryF\n",
        "    dots = np.dot(self.descs, queryF.transpose())\n",
        "    #To sort the results create a zip between dot products and ids\n",
        "    res = zip(dots, self.ids)\n",
        "    #then call the sorted function to sort them\n",
        "    res = sorted(res, reverse=True)\n",
        "    #return just the first result of the results list\n",
        "    return res[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBV9JZODXgXj"
      },
      "source": [
        "classifier = OneNNClassifier(ids, descs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTevGmHQ31L4"
      },
      "source": [
        "#TODO: STEP 4 - Video face detection & recognition\n",
        "\n",
        "def play_video(video_path, classifier):\n",
        "  frame_count = 0\n",
        "\n",
        "  print(\"[INFO] starting \" + video_path)\n",
        "  #vs = VideoStream(src=0).start() #webcam\n",
        "  vs = FileVideoStream(video_path).start()\n",
        "\n",
        "  # loop over frames from the video file stream\n",
        "  while vs.more():\n",
        "\n",
        "      # grab the frame from the threaded video stream\n",
        "      frame = vs.read()\n",
        "\n",
        "      if frame_count % FRAMES_TO_SKIP == 0:\n",
        "          bb_color = GREEN\n",
        "          detected_faces = detect(frame)\n",
        "          #foreach detected face\n",
        "          for face in detected_faces:\n",
        "            #get the image ROI\n",
        "            query = getImageROI(frame, face)\n",
        "            #extract face features\n",
        "            query_f = fe.extract(query,FEAT_LAYER,True)\n",
        "            #classify\n",
        "            confidence, prediction = classifier.predict(query_f)\n",
        "            #highlight the detected face\n",
        "            if confidence > PRED_THRESHOLD:\n",
        "              highlight(frame, face, bb_color, prediction)\n",
        "            else:\n",
        "              highlight(frame, face, RED, \"Unkown\")\n",
        "          display_img(frame)\n",
        "          \n",
        "      frame_count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALYRv0tcKVUv"
      },
      "source": [
        "play_video(VIDEO_PATH, classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqZ3eS_2RyC9"
      },
      "source": [
        "#TODO: STEP 5 -  Nearest centroid classifier\n",
        "\n",
        "mean_descs = []\n",
        "mean_ids = []\n",
        "\n",
        "#evaluate the mean feature for each class\n",
        "#set the class ids\n",
        "mean_ids = list(set(ids))\n",
        "zipped = list(zip(ids, descs))\n",
        "\n",
        "for i in range(len(mean_ids)):\n",
        "  id = mean_ids[i]\n",
        "  temp = [value for key,value in zipped if key == id]\n",
        "  mean_descs.append(np.mean(temp, axis=0))\n",
        "\n",
        "#init OneNNClassifier to make use of mean features\n",
        "nearest_centroid_classifier = OneNNClassifier(mean_ids, mean_descs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTWQI_J6KY5J"
      },
      "source": [
        "play_video(VIDEO_PATH, nearest_centroid_classifier)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}