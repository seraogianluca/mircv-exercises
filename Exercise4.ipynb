{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Exercise4_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seraogianluca/mircv-exercises/blob/main/Copia_di_Exercise4_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QTbjJ9UpGaS"
      },
      "source": [
        "# Student: Gianluca Serao\n",
        "#MIRCV 2021\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guz_8updP2T5"
      },
      "source": [
        "!pip install opencv-python==4.4.0.46"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xwwqP40pggi"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "from imutils.video import VideoStream, FileVideoStream\n",
        "\n",
        "\n",
        "BASE_DIR = '/content/gdrive/My Drive/mircv2021'\n",
        "IMG_DIR = BASE_DIR + '/data/lf_img'\n",
        "\n",
        "THRESHOLD = 35\n",
        "MIN_GOOD_MATCHES = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f4EVFw00HnI"
      },
      "source": [
        "def display_img(img, is_bgr=True):\n",
        "  if is_bgr:  # convert color from CV2 BGR to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \n",
        "  display(Image.fromarray(img))\n",
        "  clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6YyZWG0vo_Z"
      },
      "source": [
        "#TODO\n",
        "img = cv2.imread(IMG_DIR + '/figure-at-a-window.jpg')\n",
        "\n",
        "#Initialize ORB\n",
        "orb = cv2.ORB_create(1000)\n",
        "\n",
        "#Extract ORB keypoints and features from img\n",
        "kp, des = orb.detectAndCompute(img,mask=None)\n",
        "\n",
        "print(kp[0].pt)\n",
        "print(kp[0].size)\n",
        "print(kp[0].angle)\n",
        "print(kp[0].response)\n",
        "print(kp[0].octave)\n",
        "print(kp[0].class_id)\n",
        "print(des.shape)\n",
        "print(des)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tO1-zF3_i2v"
      },
      "source": [
        "#TODO\n",
        "#Draw keypoints\n",
        "out = cv2.drawKeypoints(img,kp,None)\n",
        "print('Image Keypoints')\n",
        "display_img(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhSWW0Ui_imI"
      },
      "source": [
        "#TODO\n",
        "#Draw rich keypoints\n",
        "out = cv2.drawKeypoints(img,kp,None,(0,255,255),cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "print('Image Rich Keypoints')\n",
        "display_img(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jovxjh_qyE_a"
      },
      "source": [
        "#TODO\n",
        "img1 = cv2.imread(IMG_DIR + '/2014-05-14 14.52.14.jpg')\n",
        "img2 = cv2.imread(IMG_DIR + '/20140512_105751.jpg')\n",
        "\n",
        "#Extract ORB keypoints and features from img1 and img2\n",
        "kp1, des1 = orb.detectAndCompute(img1, mask=None)\n",
        "kp2, des2 = orb.detectAndCompute(img2, mask=None)\n",
        "\n",
        "#Initialize matcher\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "#match features\n",
        "matches = bf.match(des1, des2)\n",
        "\n",
        "matches = sorted(matches, key=lambda x:x.distance)\n",
        "\n",
        "#Draw matches\n",
        "print(len(matches))\n",
        "print(matches[0].distance)\n",
        "print(matches[0].imgIdx)\n",
        "print(matches[0].queryIdx)\n",
        "print(matches[0].trainIdx)\n",
        "\n",
        "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], None)\n",
        "print('Features Matching')\n",
        "display_img(img_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKWWWzTF95aA"
      },
      "source": [
        "#TODO\n",
        "#Set a good threshold\n",
        "THRESHOLD = 35\n",
        "\n",
        "#Filter good matches\n",
        "good = [m for m in matches if m.distance<= THRESHOLD]\n",
        "print(len(good))\n",
        "\n",
        "#Draw good matches\n",
        "good_matches = cv2.drawMatches(img1, kp1, img2, kp2, good, None)\n",
        "print('Filtered Matching')\n",
        "display_img(good_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMGNWiVrEK6i"
      },
      "source": [
        "def add_bounding_box(img1, img2, kp1, kp2, good, M, mask, draw_inliers=False):\n",
        "  matchesMask = mask.ravel().tolist()\n",
        "  h, w, c = img1.shape\n",
        "  pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
        "  dst = cv2.perspectiveTransform(pts, M)\n",
        "\n",
        "  points = [np.int32(dst)]\n",
        "  img2 = cv2.polylines(img2, points, isClosed=True, color=255, thickness=3, lineType=cv2.LINE_AA)\n",
        "  if draw_inliers:\n",
        "    draw_params = dict(matchColor=(0, 255, 0), # draw matches in green color\n",
        "                       singlePointColor=None,\n",
        "                       matchesMask=matchesMask, # draw only inliers\n",
        "                       flags=2)\n",
        "    return cv2.drawMatches(img1, kp1, img2, kp2, good, None, **draw_params)\n",
        "  \n",
        "  return img2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2HoaKbYpZ9u"
      },
      "source": [
        "#TODO\n",
        "#Select the points of the best matches\n",
        "if len(good) > MIN_GOOD_MATCHES:\n",
        "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good])\n",
        "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good])\n",
        "\n",
        "#find homography\n",
        "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 1.0)\n",
        "    ransac_img = add_bounding_box(img1, img2, kp1, kp2, good, M, mask, True)\n",
        "    print('Ransac')\n",
        "    display_img(ransac_img)\n",
        "else:\n",
        "    print (\"Not enough matches are found - %d/%d\" % (len(good), MIN_GOOD_MATCHES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB5IE9uiH366"
      },
      "source": [
        "#From https://github.com/juangallostra/augmented-reality\n",
        "\n",
        "import math\n",
        "\n",
        "class OBJ:\n",
        "    def __init__(self, filename, swapyz=False):\n",
        "        \"\"\"Loads a Wavefront OBJ file. \"\"\"\n",
        "        self.vertices = []\n",
        "        self.normals = []\n",
        "        self.texcoords = []\n",
        "        self.faces = []\n",
        "        material = None\n",
        "        for line in open(filename, \"r\"):\n",
        "            if line.startswith('#'): continue\n",
        "            values = line.split()\n",
        "            if not values: continue\n",
        "            if values[0] == 'v':\n",
        "                v = list(map(float, values[1:4]))\n",
        "                if swapyz:\n",
        "                    v = v[0], v[2], v[1]\n",
        "                self.vertices.append(v)\n",
        "            elif values[0] == 'vn':\n",
        "                v = list(map(float, values[1:4]))\n",
        "                if swapyz:\n",
        "                    v = v[0], v[2], v[1]\n",
        "                self.normals.append(v)\n",
        "            elif values[0] == 'vt':\n",
        "                self.texcoords.append(map(float, values[1:3]))\n",
        "            #elif values[0] in ('usemtl', 'usemat'):\n",
        "                #material = values[1]\n",
        "            #elif values[0] == 'mtllib':\n",
        "                #self.mtl = MTL(values[1])\n",
        "            elif values[0] == 'f':\n",
        "                face = []\n",
        "                texcoords = []\n",
        "                norms = []\n",
        "                for v in values[1:]:\n",
        "                    w = v.split('/')\n",
        "                    face.append(int(w[0]))\n",
        "                    if len(w) >= 2 and len(w[1]) > 0:\n",
        "                        texcoords.append(int(w[1]))\n",
        "                    else:\n",
        "                        texcoords.append(0)\n",
        "                    if len(w) >= 3 and len(w[2]) > 0:\n",
        "                        norms.append(int(w[2]))\n",
        "                    else:\n",
        "                        norms.append(0)\n",
        "                #self.faces.append((face, norms, texcoords, material))\n",
        "                self.faces.append((face, norms, texcoords))\n",
        "\n",
        "\n",
        "def projection_matrix(camera_parameters, homography):\n",
        "  \"\"\"\n",
        "  From the camera calibration matrix and the estimated homography\n",
        "  compute the 3D projection matrix\n",
        "  \"\"\"\n",
        "  # Compute rotation along the x and y axis as well as the translation\n",
        "  homography = homography * (-1)\n",
        "  rot_and_transl = np.dot(np.linalg.inv(camera_parameters), homography)\n",
        "  col_1 = rot_and_transl[:, 0]\n",
        "  col_2 = rot_and_transl[:, 1]\n",
        "  col_3 = rot_and_transl[:, 2]\n",
        "  # normalise vectors\n",
        "  l = math.sqrt(np.linalg.norm(col_1, 2) * np.linalg.norm(col_2, 2))\n",
        "  rot_1 = col_1 / l\n",
        "  rot_2 = col_2 / l\n",
        "  translation = col_3 / l\n",
        "  # compute the orthonormal basis\n",
        "  c = rot_1 + rot_2\n",
        "  p = np.cross(rot_1, rot_2)\n",
        "  d = np.cross(c, p)\n",
        "  rot_1 = np.dot(c / np.linalg.norm(c, 2) + d / np.linalg.norm(d, 2), 1 / math.sqrt(2))\n",
        "  rot_2 = np.dot(c / np.linalg.norm(c, 2) - d / np.linalg.norm(d, 2), 1 / math.sqrt(2))\n",
        "  rot_3 = np.cross(rot_1, rot_2)\n",
        "  # finally, compute the 3D projection matrix from the model to the current frame\n",
        "  projection = np.stack((rot_1, rot_2, rot_3, translation)).T\n",
        "  return np.dot(camera_parameters, projection)\n",
        "\n",
        "\n",
        "def render(img, obj, projection, model, color=False):\n",
        "    vertices = obj.vertices\n",
        "    scale_matrix = np.eye(3) * 3\n",
        "    h, w, c = model.shape\n",
        "\n",
        "    for face in obj.faces:\n",
        "        face_vertices = face[0]\n",
        "        points = np.array([vertices[vertex - 1] for vertex in face_vertices])\n",
        "        points = np.dot(points, scale_matrix)\n",
        "        # render model in the middle of the reference surface. To do so,\n",
        "        # model points must be displaced\n",
        "        points = np.array([[p[0] + w / 2, p[1] + h / 2, p[2]] for p in points])\n",
        "        dst = cv2.perspectiveTransform(points.reshape(-1, 1, 3), projection)\n",
        "        imgpts = np.int32(dst)\n",
        "        if color is False:\n",
        "            cv2.fillConvexPoly(img, imgpts, (137, 27, 211))\n",
        "        else:\n",
        "            color = hex_to_rgb(face[-1])\n",
        "            color = color[::-1] # reverse\n",
        "            cv2.fillConvexPoly(img, imgpts, color)\n",
        "\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C2GDSl5VOrRZ"
      },
      "source": [
        "#TODO\n",
        "\n",
        "VIDEO_SRC = IMG_DIR + '/australia.mp4'\n",
        "CAMERA_SRC = 0\n",
        "FRAMES_TO_SKIP = 5\n",
        "OBJECT_IMG = IMG_DIR + '/australia.jpg'\n",
        "MIN_RANSAC_INLIERS = 12\n",
        "\n",
        "# TODO \n",
        "# read the query image OBJECT_IMG\n",
        "img_q = cv2.imread(OBJECT_IMG)\n",
        "\n",
        "# detect keypoints and extract features from query image\n",
        "kp_q, des_q = orb.detectAndCompute(img_q, mask=None)\n",
        "\n",
        "frame_count = 0\n",
        "\n",
        "#vs = VideoStream(src=0).start() #webcam\n",
        "vs = FileVideoStream(VIDEO_SRC).start()\n",
        "\n",
        "# augmented reality\n",
        "camera_parameters = np.array([[800, 0, 320], [0, 800, 240], [0, 0, 1]])\n",
        "obj = OBJ(BASE_DIR + '/data/models/fox.obj', swapyz=True)\n",
        "\n",
        "# loop over frames from the video file stream\n",
        "while vs.more():\n",
        "\n",
        "    # grab the frame from the threaded video stream\n",
        "    frame = vs.read()\n",
        "\n",
        "    if frame_count % FRAMES_TO_SKIP == 0:\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "      # TODO\n",
        "      # detect keypoints and extract features from frame\n",
        "      kp_f, des_f = orb.detectAndCompute(frame, mask=None)\n",
        "\n",
        "      # match feautures with query\n",
        "      matches_fq = bf.match(des_q, des_f)\n",
        "      matches_fq = sorted(matches_fq, key=lambda x:x.distance)\n",
        "\n",
        "      # select the good matches\n",
        "      good_fq = [m for m in matches_fq if m.distance<= THRESHOLD]\n",
        "\n",
        "      if len(good_fq) > MIN_GOOD_MATCHES:\n",
        "        query_pts = np.float32([kp_q[m.queryIdx].pt for m in good_fq])\n",
        "        frame_pts = np.float32([kp_f[m.trainIdx].pt for m in good_fq])\n",
        "\n",
        "        # ransac & findHomography\n",
        "        M_r, mask_r = cv2.findHomography(query_pts, frame_pts, cv2.RANSAC, 1.0)\n",
        "    \n",
        "        # show image with Homography and bounding box\n",
        "        ransac_im = add_bounding_box(img_q, frame, kp_q, kp_f, good_fq, M_r, mask_r, True)\n",
        "\n",
        "        # augmented reality\n",
        "        projection = projection_matrix(camera_parameters, M_r)\n",
        "        frame = render(frame, obj, projection, img_q, False)\n",
        "\n",
        "        display_img(frame)\n",
        "    frame_count += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
